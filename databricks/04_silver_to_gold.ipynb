{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b12aa8-4bf3-486f-8fb7-e7d4d384cc32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import (\n",
    "    abs, col, concat, date_format, datediff, dayofmonth, dayofweek, \n",
    "    dayofyear, expr, first, hash, last_day, lpad, lit, month, quarter, \n",
    "    row_number, sum, weekofyear, when, year\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import BooleanType, StringType, IntegerType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e74ca029-5e89-4036-bb8e-0113548c84cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = spark.conf.get(\"catalog_name\")\n",
    "schema_name = spark.conf.get(\"schema_name\")\n",
    "landing_volume_name = spark.conf.get(\"landing_volume_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "494ad64f-a6a5-41c2-af5c-d7ceaca008ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dim Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42f6c118-9920-4f2e-91ba-8da2ed1ee165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"gold_dim_time_of_day\",\n",
    "    comment=\"Time of day dimension table\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\"\n",
    "    }\n",
    ")\n",
    "def gold_dim_time_of_day():\n",
    "    # Enforce dependency on table\n",
    "    dlt.read(\"silver_bike_point\")\n",
    "\n",
    "    # Import necessary functions\n",
    "    from pyspark.sql.functions import (\n",
    "        explode, sequence, to_timestamp, hour, minute, second, \n",
    "        lpad, concat, lit, when, col, date_format\n",
    "    )\n",
    "    from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "    # Create base time range with 1440 minutes in a day (24 * 60)\n",
    "    df = spark.sql(\n",
    "        \"SELECT explode(sequence(0, 1439, 1)) as minute_of_day\"\n",
    "    )\n",
    "    \n",
    "    # Create dim_time_of_day dataframe with all the relevant columns\n",
    "    df = (df\n",
    "        .withColumn(\"hour_of_day\", (col(\"minute_of_day\") / 60).cast(IntegerType()))\n",
    "        .withColumn(\"minute_of_hour\", col(\"minute_of_day\") % 60)\n",
    "        .withColumn(\"second_of_minute\", lit(0))\n",
    "        \n",
    "        # Create time string in format HH:MM:SS\n",
    "        .withColumn(\"hour_str\", lpad(col(\"hour_of_day\").cast(StringType()), 2, \"0\"))\n",
    "        .withColumn(\"minute_str\", lpad(col(\"minute_of_hour\").cast(StringType()), 2, \"0\"))\n",
    "        .withColumn(\"second_str\", lpad(col(\"second_of_minute\").cast(StringType()), 2, \"0\"))\n",
    "        .withColumn(\"time_string\", concat(\n",
    "            col(\"hour_str\"), lit(\":\"), col(\"minute_str\"), lit(\":\"), col(\"second_str\")\n",
    "        ))\n",
    "        \n",
    "        # Create time_of_day_key in format HHMMSS\n",
    "        .withColumn(\"time_of_day_key\", concat(\n",
    "            col(\"hour_str\"), col(\"minute_str\"), col(\"second_str\")\n",
    "        ).cast(IntegerType()))\n",
    "        \n",
    "        # Add period of day\n",
    "        .withColumn(\"period_of_day\", \n",
    "            when((col(\"hour_of_day\") >= 5) & (col(\"hour_of_day\") < 12), lit(\"Morning\"))\n",
    "            .when((col(\"hour_of_day\") >= 12) & (col(\"hour_of_day\") < 17), lit(\"Afternoon\"))\n",
    "            .when((col(\"hour_of_day\") >= 17) & (col(\"hour_of_day\") < 21), lit(\"Evening\"))\n",
    "            .otherwise(lit(\"Night\"))\n",
    "        )\n",
    "        \n",
    "        # Add AM/PM indicator\n",
    "        .withColumn(\"am_pm\", when(col(\"hour_of_day\") < 12, lit(\"AM\")).otherwise(lit(\"PM\")))\n",
    "        \n",
    "        # Add 12-hour format\n",
    "        .withColumn(\"hour_12\", when(col(\"hour_of_day\") % 12 == 0, lit(12))\n",
    "                             .otherwise(col(\"hour_of_day\") % 12))\n",
    "        .withColumn(\"hour_12_str\", lpad(col(\"hour_12\").cast(StringType()), 2, \"0\"))\n",
    "        .withColumn(\"time_12_hour\", concat(\n",
    "            col(\"hour_12_str\"), lit(\":\"), col(\"minute_str\"), lit(\" \"), col(\"am_pm\")\n",
    "        ))\n",
    "        \n",
    "        # Add part of hour\n",
    "        .withColumn(\"part_of_hour\", \n",
    "            when(col(\"minute_of_hour\") < 15, lit(\"First Quarter\"))\n",
    "            .when(col(\"minute_of_hour\") < 30, lit(\"Second Quarter\"))\n",
    "            .when(col(\"minute_of_hour\") < 45, lit(\"Third Quarter\"))\n",
    "            .otherwise(lit(\"Fourth Quarter\"))\n",
    "        )\n",
    "        \n",
    "        # Add business indicators\n",
    "        .withColumn(\"is_business_hour\", \n",
    "            when((col(\"hour_of_day\") >= 9) & (col(\"hour_of_day\") < 17), lit(True))\n",
    "            .otherwise(lit(False))\n",
    "        )\n",
    "        \n",
    "        # Add rush hour indicators (typically 7-9 AM and 4-6 PM)\n",
    "        .withColumn(\"is_rush_hour\", \n",
    "            when(((col(\"hour_of_day\") >= 7) & (col(\"hour_of_day\") < 9)) | \n",
    "                 ((col(\"hour_of_day\") >= 16) & (col(\"hour_of_day\") < 18)), lit(True))\n",
    "            .otherwise(lit(False))\n",
    "        )\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        .drop(\"hour_str\", \"minute_str\", \"second_str\", \"hour_12\")\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "158e504c-43b0-4f62-b62b-6e6283da0a3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dim Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a562bf-547c-4312-a098-2ca59321da2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_dim_date\",\n",
    "    comment=\"Date dimension table\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\"\n",
    "    }\n",
    ")\n",
    "def gold_dim_date():\n",
    "    # Enforce dependency on table a\n",
    "    dlt.read(\"silver_bike_point\")\n",
    "\n",
    "    # Create base date range\n",
    "    df = spark.sql(\n",
    "        \"SELECT explode(sequence(to_date('2023-01-01'), to_date('2023-01-31'), interval 1 day)) as date\"\n",
    "    )\n",
    "    # Create dim_date dataframe with all the relevant columns\n",
    "    df = (df\n",
    "        .withColumn(\"date_key\", date_format(\"date\", \"yyyyMMdd\").cast(IntegerType()))\n",
    "        .withColumn(\"year\", year(\"date\"))\n",
    "        .withColumn(\"month_num\", month(\"date\"))\n",
    "        .withColumn(\"day_num\", dayofmonth(\"date\"))\n",
    "        .withColumn(\"day_of_week\", dayofweek(\"date\"))\n",
    "        .withColumn(\"week_of_year\", weekofyear(\"date\"))\n",
    "        .withColumn(\"day_of_year\", dayofyear(\"date\"))\n",
    "        .withColumn(\"month_name\", date_format(\"date\", \"MMMM\"))\n",
    "        .withColumn(\"month_short_name\", date_format(\"date\", \"MMM\"))\n",
    "        .withColumn(\"day_name\", date_format(\"date\", \"EEEE\"))\n",
    "        .withColumn(\"day_short_name\", date_format(\"date\", \"EEE\"))\n",
    "        .withColumn(\"quarter\", quarter(\"date\"))\n",
    "        .withColumn(\"year_month\", date_format(\"date\", \"yyyy-MM\"))\n",
    "        .withColumn(\"year_month_num\", date_format(\"date\", \"yyyyMM\"))\n",
    "        .withColumn(\"last_day_of_month\", last_day(\"date\"))\n",
    "        .withColumn(\"first_day_of_month\", expr(\"trunc(date, 'MM')\"))\n",
    "        .withColumn(\"day_of_month\", datediff(\"date\", expr(\"trunc(date, 'MM')\")) + 1)\n",
    "        .withColumn(\"is_weekend\", when(dayofweek(\"date\").isin(1, 7), lit(True)).otherwise(lit(False)))\n",
    "        .withColumn(\"is_weekday\", when(dayofweek(\"date\").isin(1, 7), lit(False)).otherwise(lit(True)))\n",
    "        .withColumn(\"season\", when(month(\"date\").isin(3, 4, 5), lit(\"Spring\"))\n",
    "                        .when(month(\"date\").isin(6, 7, 8), lit(\"Summer\"))\n",
    "                        .when(month(\"date\").isin(9, 10, 11), lit(\"Fall\"))\n",
    "                        .otherwise(lit(\"Winter\")))\n",
    "        .withColumn(\"year_str\", col(\"year\").cast(StringType()))\n",
    "        .withColumn(\"month_str\", lpad(col(\"month_num\").cast(StringType()), 2, \"0\"))\n",
    "        .withColumn(\"day_str\", lpad(col(\"day_num\").cast(StringType()), 2, \"0\"))\n",
    "        .withColumn(\"date_key\", concat(\"year_str\", \"month_str\", \"day_str\"))\n",
    "        .drop(\"year_str\", \"month_str\", \"day_str\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a3f7ceb-60ca-4207-8e09-897716e6dcce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dim Bikepoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7e3abd-760d-4faa-9396-afc8125fafeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name=\"gold_dim_bikepoint\",\n",
    "  comment=\"Bikepoint dimension table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"gold\"\n",
    "  }\n",
    ")\n",
    "def create_gold_dim_bikepoint():\n",
    "    df = spark.read.table(\"robin_huebner.tfl_analytics.silver_bike_point\")\n",
    "    window_spec = Window.partitionBy(\"bikepoint_id\").orderBy(col(\"landing_timestamp\").desc())\n",
    "    df = df.withColumn(\"row_number\", row_number().over(window_spec)).withColumn(\"bikepoint_key\", abs(hash(col(\"bikepoint_id\")))).drop(\"bikepoint_id\")\n",
    "    df = df.filter(col(\"row_number\") == 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a1751b3-7a15-40f0-b9b6-779ed3e1ad96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fact Bike Utilization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d758f2f9-8b53-4ea2-894d-581b689f4b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"gold_fact_bike_utilization_total\",\n",
    "    comment=\"The total amount of bikes which are used in a specific time\",\n",
    "    table_properties={\"quality\": \"gold\"},\n",
    ")\n",
    "def create_gold_fact_bike_utilization_total():\n",
    "    df = spark.read.table(\"robin_huebner.tfl_analytics.silver_bike_point\")\n",
    "\n",
    "    df = df.groupBy(\"bikepoint_id\", \"landing_timestamp\").agg(\n",
    "        sum(df.empty_dock_count).alias(\"empty_dock_count\"),\n",
    "        sum(df.dock_count).alias(\"dock_count\"),\n",
    "        sum(df.bike_count).alias(\"bike_count\"),\n",
    "        sum(df.standard_bike_count).alias(\"standard_bike_count\"),\n",
    "        sum(df.ebike_count).alias(\"ebike_count\"),\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"date_key\", date_format(df.landing_timestamp, \"yyyyMMdd\"))\n",
    "        .withColumn(\"time_of_day_key\", date_format(df.landing_timestamp, \"HHmmss\").cast(\"integer\"))\n",
    "        .withColumn(\"bikepoint_key\", abs(hash(col(\"bikepoint_id\"))))\n",
    "        .select(\"date_key\", \"time_of_day_key\", \"bikepoint_key\", *df.columns)\n",
    "        .drop(\"landing_timestamp\", \"bikepoint_id\")\n",
    "    )\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_silver_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
